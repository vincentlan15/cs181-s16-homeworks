
\documentclass[submit]{harvardml}

% Put in your full name and email address.
\name{Tianxing Lan}
\email{tianxinglan@college.harvard.edu}

% List any people you worked with.
\collaborators{%
  Kojin Oshiba，Dylan Ho
}

% You don't need to change these.
\course{CS181-S16}
\assignment{Assignment \#3}
\duedate{5:00pm March 25, 2016}

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage{fullpage}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{common}
\usepackage{bm}

\usepackage[mmddyyyy,hhmmss]{datetime}

\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}

\begin{document}
\begin{center}
{\Large Homework 3: SVM}\\
\end{center}

There is a mathematical component and a programming component to this homework.
Please submit ONLY your PDF to Canvas, and push all of your work to your Github
repository. If a question asks you to make any plots, like Problem 3, please
include those in the writeup.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Fitting an SVM by hand, 8pts]
Consider a dataset with the following 6 points in $1D$: \[\{(x_1, y_1)\} =\{(-3
, +1 ), (-2 , +1 ) , (-1,  -1 ), ( 1 , -1 ), ( 2 , +1 ), ( 3 , +1 )\}\] Consider
mapping these points to $2$ dimensions using the feature vector $\phi : x
\mapsto (x, x^2)$. The max-margin classifier objective is given by:
\begin{equation}
  \min_{\mathbf{w}, w_0} \|\mathbf{w}\|_2^2 \quad \text{s.t.} \quad y_i(\mathbf{w}^T c +
  w_0) \geq 1,~\forall i
\end{equation}

Note: the purpose of this exercise is to solve the SVM without the help of a
computer, relying instead on principled rules and properties of these
classifiers. The exercise has been broken down into a series of questions, each
providing a part of the solution. Make sure to follow the logical structure of
the exercise when composing your answer and to justify each step.

\begin{enumerate}
  \item Write down a vector that is parallel to the optimal vector $\mathbf{w}$. Justify
    your answer.
  \item What is the value of the margin achieved by $\mathbf{w}$? Justify your
    answer.
  \item Solve for $\mathbf{w}$ using your answers to the two previous questions.
  \item Solve for $w_0$. Justify your answer.
  \item Write down the discriminant as an explicit function of $x$.
\end{enumerate}

\end{problem}
\subsection*{Solution}
\begin{enumerate}
\item $\mathbf{w}$ needs to be parallel to (0,1). This is because all the data points of $\phi(w_i)$ are symmetric with regard to y-axis, and by the rule of symmetry, the line that separates them with the maximum margin also needs to be symmetric with regard to y axis. Specifically, for an optimal $\mathbf{w_0}$, tilting $\mathbf{w}$ from being symmetric  to y axis would either decrease its margin from (2,4) and (-3,9) or decrease its margin from (-2,4) and (3,9). Thus $\mathbf{w}$ has to be parallel to (0,1).

\item Just by looking at the graph of all $\phi{w_i}$ we can see (-3,9) and (3,9) don't matter. The boundary should be between (-2,4), (2,4) and (-1,1),(1,1), the first two with y value 1 and the latter two with y value -1. Since the boundary needs to be parallel to (1,0), it achieves maximum margin when it is 1.5 away from both (2,4),(-2,4) and (1,-1),(-1,1). Thus the margin is 1.5.

\item By law of symmetry, we can solve for $\mathbf{w}$ just using (2,4) and (1,1). From 2, both of these points restrict $\mathbf{w}$, so in both cases we have  equality. 
1($\mathbf{w}$(2,4)+$w_0$)=1
-1($\mathbf{w}$(1,1)+$w_0$)=1
$\mathbf{w}$=c(0,1)
Thus $\mathbf{w}$=(0,$\frac{2}{3}$)

\item plugging $\mathbf{w}=\frac{2}{3}$ in either of the equations and we get $w_0=-\frac{5}{3}$ 

\item $f(x)=y_i(\frac{2}{3}x^2-\frac{5}{3}-1)=y_i(\frac{2}{3}x^2-\frac{8}{3})$


\end{enumerate}



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Composing Kernel Functions , 7pts]
Prove that
\begin{align*}
	K(\boldx, \boldx') &= \exp\{ -||\boldx - \boldx'||^2_2 \}\,,
\end{align*}
where~$\boldx,\boldx'\in\reals^D$ is a valid kernel, using only the following
properties.  If~$K_1(\cdot,\cdot)$ and~$K_2(\cdot,\cdot)$ are valid kernels,
then the following are also valid kernels:
\begin{align*}
	K(\boldx, \boldx') &= c\,K_1(\boldx, \boldx') \quad \text{for $c>0$}\\
	K(\boldx, \boldx') &= K_1(\boldx, \boldx') + K_2(\boldx, \boldx')\\
	K(\boldx, \boldx') &= K_1(\boldx, \boldx')\,K_2(\boldx, \boldx')\\
	K(\boldx, \boldx') &= \exp\{ K_1(\boldx, \boldx') \}\\
  K(\boldx, \boldx') &= f(\boldx)\,K_1(\boldx, \boldx')\,f(\boldx') \quad
  \text{where $f$ is any function from~$\reals^D$ to $\reals$}
\end{align*}
 \end{problem}
\subsection*{Solution}
$K(x,x')$=$exp(-(x^Tx-2x^Tx'+x'^Tx'))=exp(-x^Tx)exp(2x^Tx')exp(x'^Tx')$
Let $f(x)=exp(-x^Tx)$, then $K(x,x')=f(x)exp(2x^Tx')f(x')$
Since $x^Tx'$ is a kernel, $2x^Tx'$ is a kernel and thus $exp(2x^Tx')$ is a kernel. Thus $K(x,x')$ is a kernel.


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[Scaling up your SVM solver, 10pts (+7pts with extra credit)]



In the previous homework, you studied a simple data set of fruit measurements.
We would like you to code up a few simple SVM solvers to classify lemons from
apples. To do this, read the paper at
\url{http://www.jmlr.org/papers/volume6/bordes05a/bordes05a.pdf} and implement
the Kernel Perceptron algorithm and the Budget Kernel Perceptron algorithm. The provided code has a base Perceptron class, which you will inherit to write KernelPerceptron and BudgetKernelPerceptron. This has been set up for you in problem3.py. The provided data is linearly separable. Make the optimization as fast as
possible. 

Additionally, we would like you to do some experimentation with the hyperparameters for each of these models. Try seeing if you can identify some patterns by changing $\beta$, N (maximum number of support vectors), or the number of random samples you take.  Note the training time, accuracy,  shapes/orientations of hyperplanes, and number of support vectors for various setups. We are intentionally leaving this open-ended to allow for experimentation, and so we will be looking for your thought process and not a rigid graph this time. That being said, any visualizations that you want us to grade and refer to in your descriptions should be included in this writeup. You can use the trivial $K(\mathbf{x_1}, \mathbf{x_2}) = \mathbf{x_1}^T\mathbf{x_2}$ kernel for this problem, though you are welcome to experiment with more interesting kernels too. Also, answer the following reading questions in one or two sentences each.

\begin{enumerate}
\item In one short sentence, state the main purpose of the paper?
\item Identify each of the parameters in Eq. 1
\item State one guarantee for the Kernel perceptron algorithm described in the
  paper.
\item What is the main way the budget kernel perceptron algorithm tries to
  improve on the perceptron algorithm.
\item In simple words, what is the theoretical guarantee of LASVM algorithm? How
  does it compare to its practical performance?
\end{enumerate}


For extra credit (+7 pts), implement the SMO algorithm and implement the LASVM process and do the same as above.




\end{problem}

\subsection*{Solution}
\begin{enumerate}
\item Using LASVM to evaluate the importance of data, and thus save more time and memory when performing high dimensional classification.  

\item w':weight vector/coefficient vector for $\phi(x)$; $\phi(x)$ feature vector resulting from x; b: bias parameter

\item Novikoff’s Theorem, which states that the perceptron algorithm converges after a finite number of mistakes, or
after inserting a finite number of support vectors. 

\item It removes support vectors from the kernel expansion to avoid  noisy data and ultimately overfitting

\item It converges to the standard SVM algorithm. It runs faster and saves memory than SVM because it has the flexibility of online algorithm and runs through the  data set only once.

\end{enumerate}

\newpage

\subsection*{Calibration [1pt]}
Approximately how long did this homework take you to complete?


\end{document}


















































